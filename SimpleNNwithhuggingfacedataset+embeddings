# fully connected NN with the hugging face data set WITH TRAINABLE EMBEDDINGS

import pandas as pd
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import re
from collections import Counter
from torch.utils.data import DataLoader, Dataset, random_split
from transformers import BertConfig, BertTokenizer, BertModel, AdamW
from torch.utils.data import DataLoader, Dataset, random_split
from datasets import load_dataset

import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Input, Dense, Embedding, Flatten
from tensorflow.keras.models import Sequential
from datasets import load_dataset
from collections import Counter
import pandas as pd
# Parameters
embed_size = 50
num_classes = 6
max_len = 5

# Tokenizer function
def tokenize(text, vocab, max_len):
    tokens = re.findall(r'\w+', text.lower())
    token_ids = [vocab.get(token, vocab['[PAD]']) for token in tokens[:max_len]]
    padding_length = max_len - len(token_ids)
    token_ids.extend([vocab['[PAD]']] * padding_length)
    return token_ids

# Load the dataset
ds = load_dataset("AdamCodd/emotion-balanced")

# Build vocabulary
def build_vocab(texts):
    vocab = {'[PAD]': 0}
    word_counter = Counter()
    for text in texts:
        tokens = re.findall(r'\w+', text.lower())
        word_counter.update(tokens)
    for word, _ in word_counter.most_common():
        vocab[word] = len(vocab)
    return vocab

# Extract the text data
texts = ds['train']['text'] + ds['validation']['text'] + ds['test']['text']

vocab = build_vocab(texts)
#vocab_size = len(vocab) + 1  # +1 for padding/index 0
vocab_size = len(vocab) 



# Tokenize the dataset
def preprocess_dataset(dataset, vocab, max_length):
    tokenized_texts = []
    labels = []
    for example in dataset:
        token_ids = tokenize(example['text'], vocab, max_len)
        tokenized_texts.append(token_ids)
        labels.append(example['label'])
    return np.array(tokenized_texts), np.array(labels)

X_train, y_train = preprocess_dataset(ds['train'], vocab, max_len)
X_val, y_val = preprocess_dataset(ds['validation'], vocab, max_len)
X_test, y_test = preprocess_dataset(ds['test'], vocab, max_len)

# Verify shapes
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Ensure y_train and other labels are integers
y_train = y_train.astype(np.int32)
y_val = y_val.astype(np.int32)
y_test = y_test.astype(np.int32)


# Define label mapping
label_mapping = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'surprise'
}

# Verify label shapes
print("y_train shape after conversion:", y_train.shape)
print("y_val shape after conversion:", y_val.shape)
print("y_test shape after conversion:", y_test.shape)



# Build the fully connected model
def build_model(vocab_size, embed_size, num_classes, max_len):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embed_size, input_length=max_len),
        tf.keras.layers.Flatten(),  # Flatten the embeddings to feed into the dense layer
        tf.keras.layers.Dense(10, activation='relu'),  # Fully connected hidden layer with 10 nodes
        tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer
    ])
    return model

# Initialize model
model = build_model(vocab_size, embed_size, num_classes, max_len)

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Example usage of prediction with a single test record
text = "things are feeling somewhat blue"
print (text)

tokens = re.findall(r'\w+', text.lower())
token_ids = [vocab.get(word, 0) for word in tokens]  # Convert tokens to token IDs
token_ids = token_ids + [0] * (max_len - len(token_ids))  # Pad the sequence
X_test_single = np.array([token_ids], dtype=np.int32)

# Prediction using the single test record
prediction = model.predict(X_test_single)
predicted_label = np.argmax(prediction)

print("Prediction array:", prediction)
print("Predicted label:", predicted_label)

# Debugging: Print the prediction array and the predicted label
print("Prediction array:", prediction)
print("Predicted label:", predicted_label)

# Directly use the label_mapping dictionary
print("Predicted emotion:", label_mapping[predicted_label])


# Check if GPU is available
if tf.config.list_physical_devices('GPU'):
    print("GPU is available.")
else:
    print("GPU is not available. TensorFlow is using the CPU.")
