# BERT MODEL
# BERT (Bidirectional Encoder Representations from Transformers) is a language representation model that is primarily used for 
#understanding the context of words in a text. Unlike models designed for generating text 
#(like GPT), BERT is optimized for tasks such as text classification, question answering, and token classification.

import pandas as pd
from datasets import load_dataset
from transformers import BertConfig, BertModel
import torch
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import re
from collections import Counter

# Load the dataset
ds = load_dataset("AdamCodd/emotion-balanced")

# Extract the text data
texts = ds['train']['text'] + ds['validation']['text'] + ds['test']['text']

# Build vocabulary
def build_vocab(texts, vocab_size=30522):
    counter = Counter()
    for text in texts:
        tokens = re.findall(r'\w+', text.lower())
        counter.update(tokens)
    most_common = counter.most_common(vocab_size - 1)  # Reserve 1 for [PAD]
    vocab = {word: idx + 1 for idx, (word, _) in enumerate(most_common)}
    vocab['[PAD]'] = 0  # Add padding token
    return vocab

vocab = build_vocab(texts)

# Tokenizer function
def tokenize(text, vocab, max_length=128):
    tokens = re.findall(r'\w+', text.lower())
    token_ids = [vocab.get(token, vocab['[PAD]']) for token in tokens[:max_length]]
    padding_length = max_length - len(token_ids)
    token_ids.extend([vocab['[PAD]']] * padding_length)
    attention_mask = [1] * len(tokens[:max_length]) + [0] * padding_length
    return token_ids, attention_mask

# Custom Dataset
class CustomDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_length=128):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        input_ids, attention_mask = tokenize(text, self.vocab, self.max_length)
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Create datasets
train_texts = ds['train']['text']
train_labels = ds['train']['label']
val_texts = ds['validation']['text']
val_labels = ds['validation']['label']
test_texts = ds['test']['text']
test_labels = ds['test']['label']

train_dataset = CustomDataset(train_texts, train_labels, vocab)
val_dataset = CustomDataset(val_texts, val_labels, vocab)
test_dataset = CustomDataset(test_texts, test_labels, vocab)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
eval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

class CustomBERT(nn.Module):
    def __init__(self, config):
        super(CustomBERT, self).__init__()
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

# Define configuration
config = BertConfig(
    vocab_size=len(vocab),
    hidden_size=256,
    num_hidden_layers=4,
    num_attention_heads=4,
    intermediate_size=512,
    hidden_act="gelu",
    hidden_dropout_prob=0.1,
    attention_probs_dropout_prob=0.1,
    max_position_embeddings=128,
    type_vocab_size=2,
    initializer_range=0.02,
    layer_norm_eps=1e-12,
    pad_token_id=0,
    gradient_checkpointing=False,
    num_labels=6  # Number of labels for classification
)

# Initialize the model
model = CustomBERT(config)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=2e-5)

# Training loop
num_epochs = 3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

# Save the model
torch.save(model.state_dict(), 'custom_bert_model.pth')

# Evaluation loop
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in eval_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        
        outputs = model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Load the model
loaded_model = CustomBERT(config)
loaded_model.load_state_dict(torch.load('custom_bert_model.pth'))
loaded_model.to(device)
loaded_model.eval()

# Define a function to predict emotions from text
def predict_emotion(text):
    input_ids, attention_mask = tokenize(text, vocab, max_length=128)
    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)
    attention_mask = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = loaded_model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, 1)
    return label_mapping[predicted.item()]

# Example usage
text = "I am feeling very happy today!"
predicted_emotion = predict_emotion(text)
print(f'Text: "{text}"\nPredicted Emotion: {predicted_emotion}')
