
#In the context of supervised learning in natural language processing (like training a Word2Vec model), a target is generally 
#required during the training phase because the model learns to predict the target word(s) based on the input context. 
#The target provides a "supervision" signal that guides the learning process by providing a measure of error between the model's 
#prediction and the actual outcome.

#1. Continuous Bag of Words (CBOW)
#Training:
#Objective: Predict the target word given a context.
#Example: Using context words ["the", "cat", "on", "the"] to predict the target word "sat".
#Hypothetical Prediction:
#Input: ["the", "cat", "on", "the"]
#Output: "sat"
#In this situation, the model would take a set of context words and attempt to predict the word that is 
#likely to appear with them based on the training data. If the phrase “the cat sat on the mat” was common in the training data, 
#the model might predict “sat” given the context words.

#2. Skip-Gram
#Training:
#bjective: Predict context words given a target word.
#Example: Using target word "sat" to predict context words ["the", "cat", "on", "the"].
#Hypothetical Prediction:
#Input: "sat"
#Output: ["the", "cat", "on", "the"]
#In this case, the model would do the opposite: given a word, it would try to predict the words that are likely to appear near it in a text. So, if “sat” is given, and if “the cat sat on the mat” is a common phrase in the training data, the model might predict context words like “the”, “cat”, "on", "mat" and so on.

import torch
import torch.nn as nn
import torch.optim as optim

# Sample data
data = [
    ("the cat sat on the mat".split(), "cat"),
    ("the dog sat on the rug".split(), "dog"),
    ("the bird flies over the trees".split(), "bird"),
    ("a mouse ran under the bed".split(), "mouse"),
    ("the sun sets behind the mountains".split(), "sun"),
    ("a fish swims in the clear pond".split(), "fish"),
    ("the moon shines bright in the night sky".split(), "moon"),
    ("the cow grazes in the lush meadow".split(), "cow"),
    ("the snake slithers through the tall grass".split(), "snake"),
    ("a spider spins a web in the corner".split(), "spider"),
    ("the lion roars in the vast savanna".split(), "lion"),
    ("a bee buzzes around the blooming flowers".split(), "bee")
]


# Vocabulary
vocab = set([word for context, target in data for word in context])
word_to_idx = {word: idx for idx, word in enumerate(vocab)}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

# Context and target
context_data = []
for context, target in data:
    context_idx = [word_to_idx[word] for word in context if word != target]
    target_idx = word_to_idx[target]
    context_data.append((context_idx, target_idx))

# Model
class CBOW(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embed_size)
        self.linear1 = nn.Linear(embed_size, 128)
        self.activation_function1 = nn.ReLU()
        self.linear2 = nn.Linear(128, vocab_size)
        self.activation_function2 = nn.LogSoftmax(dim=-1)
        
    def forward(self, inputs):
        embeds = sum(self.embeddings(inputs)).view(1, -1)
        out = self.linear1(embeds)
        out = self.activation_function1(out)
        out = self.linear2(out)
        out = self.activation_function2(out)
        return out

    def compute_loss(self, outputs, targets):
        # Example custom method to compute loss
        loss = F.nll_loss(outputs, targets)
        return loss
    
    def get_embedding(self, word_idx):
        # Example custom method to get embedding for a specific word index
        return self.embeddings(torch.tensor([word_idx], dtype=torch.long))

# Parameters
vocab_size = len(vocab)
embed_size = 10

# Initialize model, loss, optimizer
model = CBOW(vocab_size, embed_size)
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)


#When you call get_embedding before training, you will get the initial, randomly
#initialized embeddings. After training, calling get_embedding will give you the 
#updated embeddings that have been learned from the data.

embedding = model.get_embedding(1)
print("Embedding for word index 1:", embedding)

# Training loop
for epoch in range(100):
    total_loss = 0
    for context, target in context_data:
        context_var = torch.tensor(context, dtype=torch.long)
        model.zero_grad()
        log_probs = model(context_var)
        loss = loss_function(log_probs, torch.tensor([target], dtype=torch.long))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {total_loss}')

# The model's `embeddings` parameter now contains the learned word vectors

# Inference
def predict_context_word(context_words):
    context_idxs = torch.tensor([word_to_idx[w] for w in context_words], dtype=torch.long)
    log_probs = model(context_idxs)
    max_idx = torch.argmax(log_probs).item()
    return idx_to_word[max_idx]

# Example
new_context = [ "shines", "bright", "in", "the", "night", "sky"]
predicted_target = predict_context_word(new_context)
print(f'Predicted target word for context {new_context} is: "{predicted_target}"')


import torch.nn.functional as F
# Define a function to compute cosine similarity
def cosine_similarity(embedding1, embedding2):
    cos_sim = F.cosine_similarity(embedding1, embedding2)
    return cos_sim.item()

# Extract embeddings for two words
word1 = "cat"
word2 = "dog"

# Get the indices for the words
word1_idx = word_to_idx[word1]
word2_idx = word_to_idx[word2]

# Get the embeddings for the words from the model
embedding1 = model.get_embedding(word1_idx)
embedding2 = model.get_embedding(word2_idx)

# Compute the similarity score
similarity_score = cosine_similarity(embedding1, embedding2)

# Print the words, embeddings, and similarity score
print(f"Word 1: {word1}")
print(f"Embedding 1: {embedding1}")
print(f"Word 2: {word2}")
print(f"Embedding 2: {embedding2}")
print(f"Cosine Similarity: {similarity_score:.4f}")


#Extract the embeddings for the words "cat" and "dog".
#Compute the cosine similarity between the embeddings.
#Print the words, their embeddings, and the similarity score.
#The cosine_similarity function uses PyTorch’s F.cosine_similarity to calculate the similarity. The embeddings are obtained using the get_embedding method from the CBOW model, and the indices for the words are retrieved from the word_to_idx dictionary.
