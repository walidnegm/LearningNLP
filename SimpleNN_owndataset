
# A simple fully connected neural network with one hidden layer of 10 nodes with my own dataset
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import json
import nltk

# Ensure NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Load data from JSON file
with open('emotions.json', 'r') as f:
    data = json.load(f)

# Convert JSON data to DataFrame
df = pd.DataFrame(data['data'])

# Map labels to integers
label_mapping = {"happy": 0, "sad": 1, "angry": 2}
df['label'] = df['label'].map(label_mapping)

# Text Preprocessing
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    tokens = [t for t in tokens if t not in stop_words and t not in punctuation]
    return tokens[:5]  # Ensure only 5 words

df['tokens'] = df['text'].apply(preprocess_text)

# Create a vocabulary
all_tokens = [token for sublist in df['tokens'] for token in sublist]
vocab = {word: i + 1 for i, word in enumerate(sorted(Counter(all_tokens), key=Counter(all_tokens).get, reverse=True))}
vocab_size = len(vocab) + 1  # +1 for padding/index 0
max_len = 5

# Convert Tokens to Integers and Pad Sequences
df['token_ids'] = df['tokens'].apply(lambda x: [vocab[word] for word in x] + [0] * (max_len - len(x)))

# Split Data
X_train, X_test, y_train, y_test = train_test_split(df['token_ids'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)

# Convert to TensorFlow datasets
X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)

# Build the fully connected model
def build_model(vocab_size, embed_size, num_classes, max_len):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embed_size, input_length=max_len),
        tf.keras.layers.Flatten(),  # Flatten the embeddings to feed into the dense layer
        tf.keras.layers.Dense(10, activation='relu'),  # Fully connected hidden layer with 10 nodes
        tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer
    ])
    return model

# Parameters
embed_size = 50
num_classes = 3

# Initialize model
model = build_model(vocab_size, embed_size, num_classes, max_len)

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32)  # Use a reasonable batch size for efficiency
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Example usage of prediction with a single test record
text = "life is cool"
tokens = preprocess_text(text)
token_ids = [vocab.get(word, 0) for word in tokens]  # Convert tokens to token IDs
token_ids = token_ids + [0] * (max_len - len(token_ids))  # Pad the sequence
X_test_single = np.array([token_ids], dtype=np.int32)

# Prediction using the single test record
prediction = model.predict(X_test_single)
predicted_label = np.argmax(prediction)
label_mapping_inv = {v: k for k, v in label_mapping.items()}  # Inverse label mapping
print("Predicted emotion:", label_mapping_inv[predicted_label])

# Check if GPU is available
if tf.config.list_physical_devices('GPU'):
    print("GPU is available.")
else:
    print("GPU is not available. TensorFlow is using the CPU.")
