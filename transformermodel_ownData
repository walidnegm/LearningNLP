### TRANSFORMER MODEL 

import tensorflow as tf
from tensorflow.keras.layers import LayerNormalization, Dense, MultiHeadAttention, Dropout

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import json
import nltk
import tensorflow as tf

def transformer_encoder_layer(inputs, embed_dim, num_heads, ff_dim, rate=0.1, training=True):
    # Multi-Head Attention mechanism
    att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)
    att = Dropout(rate)(att, training=training)
    out1 = LayerNormalization(epsilon=1e-6)(inputs + att)  # Add & Norm
    
    # Feed-Forward Network (FFN)
    ffn_output = Dense(ff_dim, activation='relu')(out1)
    ffn_output = Dense(embed_dim)(ffn_output)
    ffn_output = Dropout(rate)(ffn_output, training=training)
    
    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)  # Add & Norm

# Example usage in the model
def build_deeper_transformer_model(vocab_size, embed_size, num_classes, max_len, num_heads, ff_dim, num_transformer_layers):
    inputs = tf.keras.Input(shape=(max_len,))
    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size)(inputs)

    # Positional Encoding
    positional_encoding = tf.keras.layers.Embedding(input_dim=max_len, output_dim=embed_size)(tf.range(start=0, limit=max_len, delta=1))
    x = embedding_layer + positional_encoding

    # Stack multiple Transformer Encoder Layers
    for _ in range(num_transformer_layers):
        x = transformer_encoder_layer(x, embed_dim=embed_size, num_heads=num_heads, ff_dim=ff_dim, rate=0.1, training=True)
    
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.Dense(128, activation='relu')(x)  # Additional dense layer
    x = tf.keras.layers.Dropout(0.5)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model



# Ensure NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Load data from JSON file
with open('emotions.json', 'r') as f:
    data = json.load(f)

# Convert JSON data to DataFrame
df = pd.DataFrame(data['data'])

# Map labels to integers
label_mapping = {"happy": 0, "sad": 1, "angry": 2}
df['label'] = df['label'].map(label_mapping)

# Text Preprocessing
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    print (tokens)
    tokens = [t for t in tokens if t not in stop_words and t not in punctuation]
    return tokens # Ensure only 5 words
    return tokens[:5]  # Ensure only 5 words

df['tokens'] = df['text'].apply(preprocess_text)

# Maximum sequence length and vocabulary size
max_len = 5
all_tokens = [token for sublist in df['tokens'] for token in sublist]
vocab = {word: i + 1 for i, word in enumerate(sorted(Counter(all_tokens), key=Counter(all_tokens).get, reverse=True))}
vocab_size = len(vocab) + 1  # +1 for padding/index 0

# Convert Tokens to Integers and Pad Sequences
df['token_ids'] = df['tokens'].apply(lambda x: [vocab[word] for word in x] + [0] * (max_len - len(x)))

# Split Data
X_train, X_test, y_train, y_test = train_test_split(df['token_ids'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)

# Convert to TensorFlow datasets
X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)


# Parameters
embed_size = 256
num_classes = 3
num_heads = 2
ff_dim = 64
num_transformer_layers = 2  # Number of transformer encoder layers

# Initialize model
print(vocab_size, embed_size, num_classes, max_len)

model = build_deeper_transformer_model(vocab_size, embed_size, num_classes, max_len, num_heads, ff_dim, num_transformer_layers)

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()


# Inspect the weights of the first Dense layer
dense_layer_weights = model.layers[-1].get_weights()
print("Dense layer weights:", dense_layer_weights)

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=1)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")
# Example usage of prediction with a single test record
text = "life is cool"
tokens = preprocess_text(text)
print("Token IDs before padding:", token_ids)
token_ids = [vocab.get(word, 0) for word in tokens]  # Convert tokens to token IDs

token_ids = token_ids + [0] * (max_len - len(token_ids))  # Pad the sequence
print("Token IDs after padding:", token_ids)

# Convert to numpy array with integer type
X_test_single = np.array([token_ids], dtype=np.int32)
print("Single test record (X_test_single):", X_test_single)

# Prediction using the single test record
prediction = model.predict(X_test_single)
predicted_label = np.argmax(prediction)
label_mapping_inv = {v: k for k, v in label_mapping.items()}  # Inverse label mapping
print("Predicted emotion:", label_mapping_inv[predicted_label])

# Check if GPU is available
if tf.config.list_physical_devices('GPU'):
    print("GPU is available.")
else:
    print("GPU is not available. TensorFlow is using the CPU.")
