#Using a small training file of my own to train a ...

#Generative pre-trained model (GPT) to predict the next best token, a transformer model generate dynamic embeddings 
#that change based on the sentence context. In this example the model will predict the next best token within the context 
#of a single sentence at a time. To handle longer contexts, such as multiple sentences in an essay, there are several strategies that can be employed.

#Encoding: The process of transforming input data (e.g., a sentence) into a different format or structure that captures meaningful 
#features. In transformers, this typically refers to the part of the model 
#that processes the input sequence to produce a hidden representation.

#Decoding: The process of transforming the encoded representation back into an output sequence. In the context of text generation, 
#decoding refers to the process of generating a sequence of words based on the encoded input. This is often done one token at a time, 
#using mechanisms like beam search or greedy search to select the most probable next token.


import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, MultiHeadAttention
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
import pandas as pd
from sklearn.model_selection import train_test_split
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import json
import nltk

def pad_sequences(sequences, maxlen, padding='post', value=0):
    padded_sequences = np.full((len(sequences), maxlen), value)
    for i, seq in enumerate(sequences):
        if len(seq) > maxlen:
            if padding == 'post':
                padded_sequences[i, :maxlen] = seq[:maxlen]
            elif padding == 'pre':
                padded_sequences[i, -maxlen:] = seq[:maxlen]
        else:
            if padding == 'post':
                padded_sequences[i, :len(seq)] = seq
            elif padding == 'pre':
                padded_sequences[i, -len(seq):] = seq
    return padded_sequences
# Positional Encoding
def positional_encoding(position, d_model):
    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
    pos_encoding = angle_rads[np.newaxis, ...]
    return tf.cast(pos_encoding, dtype=tf.float32)

def get_angles(pos, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angle_rates

# Embedding with Positional Encoding
def embedding_with_positional_encoding(vocab_size, embed_dim, max_len):
    inputs = Input(shape=(max_len,))
    x = Embedding(vocab_size, embed_dim)(inputs)
    pos_encoding = positional_encoding(max_len, embed_dim)
    x += pos_encoding[:, :max_len, :]
    return Model(inputs, x, name="embedding_with_positional_encoding")
# Transformer Encoder Layer with Unique Name
def transformer_encoder_layer(embed_dim, num_heads, ff_dim, rate=0.1, layer_num=0):
    inputs = Input(shape=(None, embed_dim))
    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)
    attn_output = Dropout(rate)(attn_output)
    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)
    
    ffn_output = Dense(ff_dim, activation='relu')(out1)
    ffn_output = Dense(embed_dim)(ffn_output)
    ffn_output = Dropout(rate)(ffn_output)
    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)
    
    return Model(inputs, out2, name=f"transformer_encoder_layer_{layer_num}")

# Transformer Decoder Layer with Unique Name
def transformer_decoder_layer(embed_dim, num_heads, ff_dim, rate=0.1, layer_num=0):
    inputs = Input(shape=(None, embed_dim))
    enc_output = Input(shape=(None, embed_dim))
    
    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)
    attn1 = Dropout(rate)(attn1)
    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn1)
    
    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(out1, enc_output)
    attn2 = Dropout(rate)(attn2)
    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)
    
    ffn_output = Dense(ff_dim, activation='relu')(out2)
    ffn_output = Dense(embed_dim)(ffn_output)
    ffn_output = Dropout(rate)(ffn_output)
    out3 = LayerNormalization(epsilon=1e-6)(out2 + ffn_output)
    
    return Model([inputs, enc_output], out3, name=f"transformer_decoder_layer_{layer_num}")

# Complete Transformer Model with Unique Layer Names
def build_transformer(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len):
    # Inputs
    enc_inputs = Input(shape=(max_len,))
    dec_inputs = Input(shape=(max_len,))
    
    # Embedding layers
    embedding_layer = embedding_with_positional_encoding(vocab_size, embed_dim, max_len)
    enc_embedding = embedding_layer(enc_inputs)
    dec_embedding = embedding_layer(dec_inputs)
    
    # Encoder layers
    enc_output = enc_embedding
    for i in range(num_layers):
        enc_output = transformer_encoder_layer(embed_dim, num_heads, ff_dim, layer_num=i)(enc_output)
    
    # Decoder layers
    dec_output = dec_embedding
    for i in range(num_layers):
        dec_output = transformer_decoder_layer(embed_dim, num_heads, ff_dim, layer_num=i)([dec_output, enc_output])
    
    # Final dense layer
    outputs = Dense(vocab_size, activation='softmax')(dec_output)
    
    return Model([enc_inputs, dec_inputs], outputs, name="transformer")

# Load data from JSON file
with open('emotions.json', 'r') as f:
    data = json.load(f)

# Convert JSON data to DataFrame
import pandas as pd
df = pd.DataFrame(data['data'])

# Determine vocabulary size based on dataset
tokenizer = Tokenizer(filters='', lower=True, oov_token='<OOV>')
tokenizer.fit_on_texts(df['text'].values)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1  # Add 1 for padding/oov token

embed_dim = 256
num_heads = 8
ff_dim = 512
num_layers = 4
max_len = 5
batch_size = 1
epochs = 20

tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=True, oov_token='<OOV>')
tokenizer.fit_on_texts(df['text'].values)

# Create sequences
sequences = tokenizer.texts_to_sequences(df['text'].values)

# Pad sequences
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')
#enc_inputs: These sequences are used as input to the encoder.
#dec_inputs: These sequences are used as input to the decoder. They are typically the same as enc_inputs, but shifted by one position.
#labels: These sequences are the target labels for training, which are also shifted versions of enc_inputs.

# Create input and output sequences
enc_inputs = padded_sequences
dec_inputs = np.zeros_like(padded_sequences)
dec_inputs[:, 1:] = padded_sequences[:, :-1]
dec_inputs[:, 0] = tokenizer.word_index['<OOV>']

# Labels are the same as the decoder inputs shifted by one
labels = np.zeros_like(padded_sequences)
labels[:, :-1] = padded_sequences[:, 1:]
labels[:, -1] = tokenizer.word_index['<OOV>']

# Create TensorFlow dataset

dataset = tf.data.Dataset.from_tensor_slices((enc_inputs, dec_inputs, labels)).shuffle(10000).batch(batch_size)


# Build the model
transformer_model = build_transformer(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len)
transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Summary of the model
transformer_model.summary()

# Example training loop (simplified)
# Training the Transformer Model
def train_transformer(model, dataset, epochs):
    for epoch in range(epochs):
        for (batch, (enc_inputs, dec_inputs, labels)) in enumerate(dataset):
            with tf.GradientTape() as tape:
                predictions = model([enc_inputs, dec_inputs], training=True)
                loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)
            
            gradients = tape.gradient(loss, model.trainable_variables)
            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            
            if batch % 100 == 0:
                print(f"Epoch {epoch + 1} Batch {batch} Loss {tf.reduce_mean(loss).numpy()}")

# Train the model
train_transformer(transformer_model, dataset, epochs)

 # Function to generate text
def generate_text(model, tokenizer, input_text, max_len):
    enc_input = tokenizer.texts_to_sequences([input_text])
    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, maxlen=max_len, padding='post')
    
    dec_input = np.zeros((1, max_len))
    dec_input[0, 0] = tokenizer.word_index['<OOV>']
    
    for i in range(1, max_len):
        predictions = model([enc_input, dec_input], training=False)
        predicted_id = tf.argmax(predictions[0, i-1]).numpy()
        dec_input[0, i] = predicted_id
        
        if predicted_id == tokenizer.word_index['<OOV>']:
            break
    
    generated_text = ' '.join([tokenizer.index_word[idx] for idx in dec_input[0] if idx != 0])
    return generated_text


# Example usage of text generation
