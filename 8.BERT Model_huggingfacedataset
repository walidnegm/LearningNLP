# BERT MODEL
# BERT (Bidirectional Encoder Representations from Transformers) is a language representation model that is primarily used for 
#understanding the context of words in a text. Unlike models designed for generating text 
#(like GPT), BERT is optimized for tasks such as text classification, question answering, and token classification.

import pandas as pd
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import re
from collections import Counter
from torch.utils.data import DataLoader, Dataset, random_split
from transformers import BertConfig, BertTokenizer, BertModel, AdamW
from torch.utils.data import DataLoader, Dataset, random_split
from datasets import load_dataset



# Load the dataset
ds = load_dataset("AdamCodd/emotion-balanced")

# Extract the text data
texts = ds['train']['text'] + ds['validation']['text'] + ds['test']['text']

# Build vocabulary
def build_vocab(texts, vocab_size=30522):
    counter = Counter()
    for text in texts:
        tokens = re.findall(r'\w+', text.lower())
        counter.update(tokens)
    most_common = counter.most_common(vocab_size - 1)  # Reserve 1 for [PAD]
    vocab = {word: idx + 1 for idx, (word, _) in enumerate(most_common)}
    vocab['[PAD]'] = 0  # Add padding token
    return vocab

vocab = build_vocab(texts)

# Tokenizer function
def tokenize(text, vocab, max_length=128):
    tokens = re.findall(r'\w+', text.lower())
    token_ids = [vocab.get(token, vocab['[PAD]']) for token in tokens[:max_length]]
    padding_length = max_length - len(token_ids)
    token_ids.extend([vocab['[PAD]']] * padding_length)
    attention_mask = [1] * len(tokens[:max_length]) + [0] * padding_length
    return token_ids, attention_mask

# Custom Dataset
class CustomDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_length=128):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        input_ids, attention_mask = tokenize(text, self.vocab, self.max_length)
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'label': torch.tensor(label, dtype=torch.long)
        }

tokenized_datasets = ds.map(tokenize_function, batched=True)

# Format the dataset for PyTorch
tokenized_datasets.set_format(type='torch', columns=['input_ids', 'label'])
train_dataset = tokenized_datasets['train']
eval_dataset = tokenized_datasets['validation']
test_dataset = tokenized_datasets['test']


# Custom BERT Model
class CustomBERT(nn.Module):
    def __init__(self, config):
        super(CustomBERT, self).__init__()
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    
    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

#Define configuration
config = BertConfig(
    vocab_size=len(vocab),
    hidden_size=256,
    num_hidden_layers=4,
    num_attention_heads=4,
    intermediate_size=512,
    hidden_act="gelu",
    hidden_dropout_prob=0.1,
    attention_probs_dropout_prob=0.1,
    max_position_embeddings=128,
    type_vocab_size=2,
    initializer_range=0.02,
    layer_norm_eps=1e-12,
    pad_token_id=0,
    gradient_checkpointing=False,
    num_labels=6  # Number of labels for classification
)

# Initialize the model
model = CustomBERT(config)


# Interactive prompt
if os.path.exists('custom_bert_model.pth'):
    choice = input("A pre-trained model exists. Do you want to use the existing model or retrain? (use/retrain): ").strip().lower()
else:
    choice = "retrain"

if choice == "use":
    # Load the model
    loaded_model = CustomBERT(config)
    loaded_model.load_state_dict(torch.load('custom_bert_model.pth'))
    loaded_model.to(device)
    loaded_model.eval()
    model = loaded_model
    print("Using the existing model.")

else: 
    
    # Create a smaller subset for faster testing
    train_subset_size = 10000
    train_subset, _ = random_split(train_dataset, [train_subset_size, len(train_dataset) - train_subset_size])

    # DataLoader
    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)
    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)
    
    # Loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=2e-5)

    # Training loop with fewer epochs and batch size
    num_epochs = 10  # Reduce the number of epochs for faster testing
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in train_loader:
            optimizer.zero_grad()
            input_ids = batch['input_ids'].to(device)
            attention_mask = (input_ids != 0).to(device)  # Create attention mask on the fly
            labels = batch['label'].to(device)
        
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        
            total_loss += loss.item()
        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

    # Save the model
    torch.save(model.state_dict(), 'custom_bert_model.pth')

# Evaluation loop
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in eval_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = (input_ids != 0).to(device)  # Create attention mask on the fly
        labels = batch['label'].to(device)
        
        outputs = model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Load the model
loaded_model = CustomBERT(config)
loaded_model.load_state_dict(torch.load('custom_bert_model.pth'))
loaded_model.to(device)
loaded_model.eval()
# Define the label mapping
label_mapping = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'surprise'
}

# Define a function to predict emotions from text
def predict_emotion(text):
    input_ids = torch.tensor([[vocab.get(word, 0) for word in text.split()[:128]] + [0] * (128 - len(text.split()[:128]))], dtype=torch.long).to(device)
    attention_mask = (input_ids != 0).to(device)
    with torch.no_grad():
        outputs = loaded_model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, 1)
    return label_mapping.get(predicted.item(), "Unknown")
    
# Example usage
text = "i feel ok and tired because i woke early"
predicted_emotion = predict_emotion(text)
print(f'Text: "{text}"\nPredicted Emotion: {predicted_emotion}')
