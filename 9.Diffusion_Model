## Diffuion model

import os
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from diffusers import DDPMPipeline, DDPMScheduler
from diffusers.models import UNet2DModel
import pickle
import numpy as np
import os
import torch
from torch.utils.data import Dataset, DataLoader
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from diffusers import DDPMPipeline, DDPMScheduler
from diffusers.models import UNet2DModel
def unpickle(file):
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict


def load_cifar10(data_dir):
    # Load all training batches
    train_data = []
    train_labels = []
    for i in range(1, 6):
        batch = unpickle(os.path.join(data_dir, f'data_batch_{i}'))
        train_data.append(batch[b'data'])
        train_labels += batch[b'labels']
    
    train_data = np.concatenate(train_data)
    train_data = train_data.reshape(-1, 3, 32, 32).astype(np.float32)
    train_labels = np.array(train_labels)

    # Load test batch
    test_batch = unpickle(os.path.join(data_dir, 'test_batch'))
    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).astype(np.float32)
    test_labels = np.array(test_batch[b'labels'])

    return (train_data, train_labels), (test_data, test_labels)

# Path to CIFAR-10 data directory
data_dir = 'cifar-10-python/cifar-10-batches-py'  # Corrected path
(train_data, train_labels), (test_data, test_labels) = load_cifar10(data_dir)

class CIFAR10Dataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        image = self.data[idx]
        label = self.labels[idx]
        image = image.transpose((1, 2, 0))  # Change from (C, H, W) to (H, W, C)
        if self.transform:
            image = self.transform(image)
        return image, label

# Example transformation
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert to tensor
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize
])

# Create datasets
train_dataset = CIFAR10Dataset(train_data, train_labels, transform=transform)
test_dataset = CIFAR10Dataset(test_data, test_labels, transform=transform)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# Define the UNet model
model = UNet2DModel(
    in_channels=3,  # 3 channels for RGB images
    out_channels=3,
    layers_per_block=2,
    block_out_channels=(64, 128, 256, 512),
    down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D", "AttnDownBlock2D"),
    up_block_types=("AttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D"),
)

# Define the scheduler
scheduler = DDPMScheduler(num_train_timesteps=1000)

# Define the pipeline
pipeline = DDPMPipeline(unet=model, scheduler=scheduler)

# Training loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(10):  # Number of epochs
    for i, (images, _) in enumerate(train_loader):
        images = images.to(device)  # Shape: (batch_size, 3, 32, 32)
        batch_size = images.shape[0]
        
        noise = torch.randn_like(images).to(device)  # Shape: (batch_size, 3, 32, 32)
        timesteps = torch.randint(0, scheduler.num_train_timesteps, (batch_size,), device=device).long()  # Shape: (batch_size,)

        # Forward pass
        noisy_images = scheduler.add_noise(images, noise, timesteps)  # Shape: (batch_size, 3, 32, 32)
        model_output = model(noisy_images, timesteps).sample  # Shape: (batch_size, 3, 32, 32)

        # Calculate loss
        loss = torch.nn.functional.mse_loss(model_output, noise)  # Loss between predicted noise and actual noise

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if i % 100 == 0:
            print(f"Epoch {epoch}, Step {i}, Loss: {loss.item()}")

# Save the model
model.save_pretrained("diffusion_model")
scheduler.save_pretrained("diffusion_scheduler")



