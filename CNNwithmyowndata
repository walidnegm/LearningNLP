### CNN WITH EMBEDDING

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import json
import nltk

# Ensure NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Load data from JSON file
with open('emotions.json', 'r') as f:
    data = json.load(f)

# Convert JSON data to DataFrame
df = pd.DataFrame(data['data'])

# Map labels to integers
label_mapping = {"happy": 0, "sad": 1, "angry": 2}
df['label'] = df['label'].map(label_mapping)

# Text Preprocessing
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    print (tokens)
    tokens = [t for t in tokens if t not in stop_words and t not in punctuation]
    return tokens # Ensure only 5 words
    return tokens[:5]  # Ensure only 5 words

df['tokens'] = df['text'].apply(preprocess_text)

all_tokens = [token for sublist in df['tokens'] for token in sublist]
vocab = {word: i + 1 for i, word in enumerate(sorted(Counter(all_tokens), key=Counter(all_tokens).get, reverse=True))}
vocab_size = len(vocab) + 1  # +1 for padding/index 0

# Convert Tokens to Integers and Pad Sequences
df['token_ids'] = df['tokens'].apply(lambda x: [vocab[word] for word in x] + [0] * (max_len - len(x)))

# Split Data
X_train, X_test, y_train, y_test = train_test_split(df['token_ids'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)

# Convert to TensorFlow datasets
X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)


# Build the model
def build_model(vocab_size, embed_size, num_classes, max_len, num_filters_first_layer, filter_width, filter_height):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embed_size, input_length=max_len),
        tf.keras.layers.Reshape((max_len, embed_size, 1)),  # Add channel dimension
    
       # First hidden layer
        tf.keras.layers.Conv2D(num_filters_first_layer, (filter_height, filter_width), activation='relu', padding='same'),
        
        # Second hidden layer
        tf.keras.layers.Conv2D(num_filters_first_layer , (filter_height, filter_width), activation='relu', padding='same'),  # Add this layer
        
 
        tf.keras.layers.MaxPooling2D(pool_size=(2, 1)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model

# Parameters
embed_size = 50
num_classes = 3
num_filters_first_layer = 10
filter_width = 2
filter_height = 2

# Initialize model
print (vocab_size, embed_size, num_classes, max_len)

model = build_model(vocab_size, embed_size, num_classes, max_len, num_filters_first_layer, filter_width, filter_height)

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Inspect the weights of the first Dense layer
dense_layer_weights = model.layers[-1].get_weights()
print("Dense layer weights:", dense_layer_weights)

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=1)
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")
# Example usage of prediction with a single test record
text = "life is cool"
tokens = preprocess_text(text)
print("Token IDs before padding:", token_ids)
token_ids = [vocab.get(word, 0) for word in tokens]  # Convert tokens to token IDs

token_ids = token_ids + [0] * (max_len - len(token_ids))  # Pad the sequence
print("Token IDs after padding:", token_ids)

# Convert to numpy array with integer type
X_test_single = np.array([token_ids], dtype=np.int32)
print("Single test record (X_test_single):", X_test_single)

# Prediction using the single test record
prediction = model.predict(X_test_single)
predicted_label = np.argmax(prediction)
label_mapping_inv = {v: k for k, v in label_mapping.items()}  # Inverse label mapping
print("Predicted emotion:", label_mapping_inv[predicted_label])

# Check if GPU is available
if tf.config.list_physical_devices('GPU'):
    print("GPU is available.")
else:
    print("GPU is not available. TensorFlow is using the CPU.")
